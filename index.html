<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="VMDT">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Trustworthiness, Video modality, Foundation models">
  <!-- TODO: List all authors -->
  <meta name="author" content="Yujin Potter, Zhun Wang, Nicholas Crispino, Kyle Montgomery, Alexander Xiong, Ethan Y. Chang, Francesco Pinto, Yuqi Chen, Rahul Gupta, Morteza Ziyadi, Christos Christodoulopoulos, Bo Li, Chenguang Wang, Dawn Song">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="blog">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="VMDT">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness.">
  <!-- TODO: Replace with your actual website URL -->
  <!--<meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">-->
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <!--<meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">-->
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Yujin Potter">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Trustworthiness">
  <meta property="article:tag" content="Video modality">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="VMDT - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="VMDT">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Advances in Neural Information Processing Systems">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>VMDT</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/vmdt.png">
  <link rel="apple-touch-icon" href="static/images/vmdt.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "VMDT",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/vmdt.png",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">VMDT: Decoding the Trustworthiness of Video Foundation Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <strong><span class="author-block">Yujin Potter<sup>1*</sup>,</span>
              <span class="author-block">Zhun Wang<sup>1*</sup>,</span>
			  <span class="author-block">Nicholas Crispino<sup>2*</sup>,</span>
			  <span class="author-block">Kyle Montgomery<sup>2*</sup>,</span>
			  <span class="author-block">Alexander Xiong<sup>1*</sup>,</span>
			  <span class="author-block">Ethan Y. Chang<sup>3</sup>,</span>
			  <span class="author-block">Francesco Pinto<sup>4</sup>,</span>
			  <span class="author-block">Yuqi Chen<sup>2</sup>,</span>
			  <span class="author-block">Rahul Gupta<sup>5</sup>,</span>
			  <span class="author-block">Morteza Ziyadi<sup>5</sup>,</span>
			  <span class="author-block">Christos Christodoulopoulos<sup>6</sup>,</span>
			  <span class="author-block">Bo Li<sup>4</sup>,</span>
			  <span class="author-block">Chenguang Wang<sup>2</sup>,</span>
			  <span class="author-block">Dawn Song<sup>1</sup></span></strong>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup> University of California, Berkeley<br>
					<sup>2</sup> University of California, Santa Cruz<br>
					<sup>3</sup> University of Illinois at Urbana-Champaign<br>
					<sup>4</sup> University of Chicago<br>
					<sup>5</sup> Amazon<br>
					<sup>6</sup> Information Commissioner's Office<br><br>
					<strong>NeurIPS 2025 Datasets & Benchmarks</strong> </span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Lead Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/sunblaze-ucb/VMDT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Data</span>
                  </a>
                </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            As foundation models become more sophisticated, ensuring their trustworthiness becomes increasingly critical; yet, unlike text and image, the video modality still lacks comprehensive trustworthiness benchmarks. We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness. Through our extensive evaluation of 7 T2V models and 19 V2T models using VMDT, we uncover several significant insights. For instance, all open-source T2V models evaluated fail to recognize harmful queries and often generate harmful videos, while exhibiting higher levels of unfairness compared to image modality models. In V2T models, unfairness and privacy risks rise with scale, whereas hallucination and adversarial robustness improve—though overall performance remains low. Uniquely, safety shows no correlation with model size, implying that factors other than scale govern current safety levels. Our findings highlight the urgent need for developing more robust and trustworthy video foundation models, and VMDT provides a systematic framework for measuring and tracking progress toward this goal.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Example image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
	<h2 class="title is-3">Examples</h2>
      <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/vmdt-main.png" alt="Examples" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Examples of untrustworthy model responses for each perspective
        </h2>
      </div>
</div>
</div>
</section>
<!-- End example image -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Each Aspect and Findings</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item content-1">
          <div class="content">
		  <h3 class="subtitle">Safety</h3>
            <div class="columns is-vcentered">
			  <!-- Image column -->
			  <div class="column is-half">
				<figure class="image is-3by3">
				  <img src="static/images/safety.png" alt="safety" loading="lazy" />
				</figure>
				<figcaption class="has-text-centered is-size-7">
				<br>
					Average harmful content generation rate (HGR) for evaluating the safety of V2T models. Different model families are represented by distinct colors.
				  </figcaption>
			  </div>

			  <!-- Text column -->
			  <div class="column is-half">
				<p>
				  We construct a comprehensive safety evaluation dataset comprising 780 prompts for T2V models and 990 prompts for V2T models, spanning 13 and 27 risk categories, respectively.
				  Our risk taxonomy is grounded in established industry policies and benchmarks, while also addressing the unique characteristics of the video modality such as temporal and physical harm risks that cannot be detected in static frames.
				  We design novel scenarios to test the performance of models under diverse conditions, including transformed instructions, synthetic content, and real-world content.
				  Our evaluation and analysis reveal several critical findings:
				  (1) Open-source T2V models universally lack safety refusal mechanisms, while even closed-source models struggle with video-specific risks like temporal and physical harm.
				  (2) T2V models generate less harmful content in transformed scenarios, likely reflecting capability limitations rather than improved safety.
				  (3) A substantial safety gap exists between open and closed-source V2T models, with all open-source variants demonstrating significantly higher harmful content generation rates.
				  (4) Closed-source V2T models like Claude and GPT exhibit better safety overall but remain particularly vulnerable to fraud and deception risks, highlighting critical alignment gaps across all VFMs.
				</p>
			  </div>
			  </div>
          </div>
        </div>
		
		<div class="item content-2">
          <div class="content">
		  <h3 class="subtitle">Hallucination</h3>
            <div class="columns is-vcentered">
			  <!-- Image column -->
			  <div class="column is-half">
				<figure class="image is-3by3">
				  <img src="static/images/hallucination.png" alt="hallucination" loading="lazy" />
				  <figcaption class="has-text-centered is-size-7">
					Average accuracy of V2T models over all hallucination scenarios as a function of model size. Different model families are represented by distinct colors. Within model families, performance tends to increase as model size increases.
				  </figcaption>
				</figure>
			  </div>

			  <!-- Text column -->
			  <div class="column is-half">
			<p>
              We construct a diverse dataset comprising 1,650 prompts for T2V models and 1,218 prompts for V2T models with the aim of measuring hallucination under different scenarios. 
			Our hallucination dataset incorporates various scenarios including naturally difficult prompts, distraction, misleading, counterfactual reasoning, temporal activities, and OCR.
			We evaluate these scenarios across a set of tasks focusing on objects, attributes, actions, counting, and spatial understanding, as well as a scene understanding task for V2T. 
			Our analysis reveals the following: 1) For T2V, all evaluated open-source models hallucinate significantly more than closed-source models, Luma and Pika, in nearly all scenarios. 2) Object recognition is the easiest task for T2V models, while OCR presents one of the most challenging scenarios. This aligns with the hallucination results observed in text-to-image (T2I) models, suggesting that T2V and T2I models share common challenges. 3) Within the same model class, an increase in V2T model size is associated with a decrease in hallucination. 4) For V2T models, we find the best-performing model on average is InternVL2.5-78B, an open-source model, which is the opposite of what is seen in T2V models.
            </p>
			</div>
			</div>
          </div>
        </div>
        <div class="item content-3">
          <div class="content">
		  <h3 class="subtitle">Fairness</h3>
		  <div class="columns is-vcentered">
			  <!-- Image column -->
			  <div class="column is-half">
				<figure class="image is-4by3">
				  <img src="static/images/fairness.png" alt="fairness" loading="lazy" />
				  <figcaption class="has-text-centered is-size-7">
					Age stereotype score of V2T models by size. The scores range from -1 to 1, where positive values indicate stereotypes associating older people with higher socioeconomic status, negative values associate younger people with higher status, and 0 represents perfect fairness. The figure shows larger models demonstrate stronger stereotypical associations between older age and higher socioeconomic status. Model families are distinguished by color.
				  </figcaption>
				</figure>
			  </div>

			  <!-- Text column -->
			  <div class="column is-half">
            <p>
              We construct an extensive dataset comprising 1,086 prompts for T2V models and 5,008 prompts for V2T models. This fairness dataset aims to assess model fairness across various contexts, including social stereotypes (e.g., occupation) and decision-making scenarios (e.g., hiring). We also examine "overkill fairness," where models sacrifice factual/historical accuracy in pursuit of diversity (e.g., generating videos of Black female Founding Fathers). Our evaluation reveals several significant findings: 1) T2V models exhibit substantial overrepresentation towards males, White individuals, and younger people, while demonstrating some degree of overkill fairness. 2) This overrepresentation surpasses that of T2I models, yet shows lower levels of overkill fairness, suggesting a trade-off between these two dimensions. 3) V2T model fairness demonstrates a significant negative correlation with model size, with larger models exhibiting increased unfairness. 4) All V2T models show significant overkill fairness, generating historically inaccurate outputs to promote diversity.
            </p>
			</div>
			</div>
          </div>
        </div>
		<div class="item content-4">
          <div class="content">
		  <h3 class="subtitle">Privacy</h3>
		  <div class="columns is-vcentered">
			  <!-- Image column -->
			  <div class="column is-half">
				<figure class="image is-3by3">
				  <img src="static/images/privacy.png" alt="privacy" loading="lazy" />
				  <figcaption class="has-text-centered is-size-7">
					A scatter plot between location inference accuracy and model size. This suggests that larger models generally demonstrate greater precision in identifying specific locations, indicating elevated privacy risks.
				  </figcaption>
				</figure>
			  </div>

			  <!-- Text column -->
			  <div class="column is-half">
            <p>
               We construct a balanced dataset of 1,000 text prompts for T2V models and 200 video samples for V2T models to evaluate the privacy memorization and extractive capabilities, respectively. Our T2V dataset comprises text prompts sampled from a pretraining corpus (i.e., caption-video pairs) used for most contemporary T2V models. Our V2T dataset comprises driving scene videos along with their location information (e.g., zip code) to evaluate inference capabilities to predict sensitive location data. Our evaluation results reveal the following:
				1) T2V models generally exhibit weak data memorization.
				2) However, we observe that the T2V VideoCrafter2 model sometimes includes watermarks from copyrighted training data in its generated videos, indicating some level of data memorization does occur.
				3) Larger V2T models tend to demonstrate stronger location inference, suggesting that privacy risks increase as model size increases. 
            </p>
			</div>
			</div>
          </div>
        </div>
		<div class="item content-5">
          <div class="content">
		  <h3 class="subtitle">Adversarial Robustness</h3>
            <figure class="image">
				  <img src="static/images/adv_robustness.PNG" alt="adv" loading="lazy" />
				  <figcaption class="has-text-centered is-size-7">
					Left: Overall benign accuracy as a function of model size. Middle: Overall robust accuracy as a function of model size. Right: Overall performance drop (benign - adversarial) as a function of
					model size.
				  </figcaption>
				</figure>
			<p><br>
              We construct a challenging dataset to assess the robustness of T2V and V2T models to adversarial inputs. Our dataset contains 329 prompts for T2V models and 1,523 prompts for V2T models across five tasks: action recognition, attribute recognition, counting, object recognition, and spatial understanding. By attacking selected T2V and V2T surrogate models, we adversarially optimize the inputs. Our findings reveal several important insights. 
				1) Both T2V and V2T models are vulnerable to adversarial inputs.
				2) Among our five tasks, counting and spatial understanding pose the greatest challenge for both T2V and V2T models. 
				3) The performance gap between open and closed-source T2V models is larger than that of V2T models. 
				4) Within the same V2T model class, larger models generally demonstrate greater robustness to adversarial inputs than their smaller counterparts.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- leaderboard -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Leaderboard</h2>

      <!-- Leaderboard table (replaces the PDF iframe) -->
<style>
  /* keep styles local to this section */
  .leaderboard-table .blue { color: #2563eb; font-weight: 600; }   /* Bulma-ish blue */
  .leaderboard-table .red  { color: #dc2626; font-weight: 600; }   /* Bulma-ish red  */
  .leaderboard-table caption { caption-side: top; text-align: left; font-weight: 600; }
  .leaderboard-table small { color: #6b7280; }
</style>

<div class="table-container leaderboard-table">
  <table class="table is-striped is-hoverable is-fullwidth is-narrow">
    <caption>
      Trustworthiness profiles for each model across five perspectives.
      <small>(Best average in <span class="blue">blue</span>, worst in <span class="red">red</span> where indicated.)</small>
    </caption>
    <thead>
      <tr>
        <th>Model Type</th>
        <th>Model Family</th>
        <th>Model Name</th>
        <th>Safety</th>
        <th>Hallucination</th>
        <th>Fairness</th>
        <th>Privacy</th>
        <th>Adv Robustness</th>
        <th>Average</th>
      </tr>
    </thead>
    <tbody>
      <!-- T2V -->
      <tr>
        <td rowspan="7">T2V</td>
        <td>VideoCrafter</td>
        <td>VideoCrafter2</td>
        <td>76.2</td><td>35.7</td><td>60.5</td><td>65.7</td><td>50.9</td><td>57.8</td>
      </tr>
      <tr>
        <td>CogVideoX</td>
        <td class="red">CogVideoX-5B</td>
        <td class="red">64.1</td><td class="red">37.8</td><td class="red">59.5</td>
        <td class="red">66.2</td><td class="red">50.9</td><td class="red">55.7</td>
      </tr>
      <tr>
        <td>OpenSora</td>
        <td>OpenSora 1.2</td>
        <td>66.8</td><td>37.1</td><td>54.6</td><td>65.8</td><td>59.3</td><td>56.72</td>
      </tr>
      <tr>
        <td>Vchitect</td>
        <td>Vchitect-2.0</td>
        <td>67.4</td><td>49.0</td><td>62.2</td><td>65.8</td><td>64.0</td><td>61.68</td>
      </tr>
      <tr>
        <td>Luma</td>
        <td class="blue">Luma</td>
        <td class="blue">83.3</td><td class="blue">67.6</td><td class="blue">57.3</td>
        <td class="blue">65.1</td><td class="blue">77.2</td><td class="blue">70.1</td>
      </tr>
      <tr>
        <td>Pika</td>
        <td>Pika</td>
        <td>59.9</td><td>63.0</td><td>51.84</td><td>64.4</td><td>71.0</td><td>62.028</td>
      </tr>
      <tr>
        <td>Nova</td>
        <td>Nova Reel</td>
        <td>90.4</td><td>45.9</td><td>62.06</td><td>66.4</td><td>75.4</td><td>68.032</td>
      </tr>

      <!-- V2T -->
      <tr>
        <td rowspan="19">V2T</td>
        <td rowspan="7">InternVL2.5</td>
        <td>InternVL2.5-1B</td>
        <td>54.1</td><td>38.2</td><td>82.0</td><td>89.1</td><td>72.1</td><td>67.1</td>
      </tr>
      <tr>
        <td>InternVL2.5-2B</td>
        <td>50.6</td><td>47.3</td><td>76.7</td><td>82.6</td><td>77.3</td><td>66.9</td>
      </tr>
      <tr>
        <td>InternVL2.5-4B</td>
        <td>46.1</td><td>52.3</td><td>76.0</td><td>82.4</td><td>80.5</td><td>67.46</td>
      </tr>
      <tr>
        <td>InternVL2.5-8B</td>
        <td>47.5</td><td>51.8</td><td>82.1</td><td>77.9</td><td>84.4</td><td>68.74</td>
      </tr>
      <tr>
        <td>InternVL2.5-26B</td>
        <td>50.6</td><td>60.4</td><td>78.6</td><td>75.5</td><td>86.2</td><td>70.26</td>
      </tr>
      <tr>
        <td>InternVL2.5-38B</td>
        <td>47.9</td><td>64.4</td><td>79.2</td><td>73.6</td><td>89.6</td><td>70.94</td>
      </tr>
      <tr>
        <td class="blue">InternVL2.5-78B</td>
        <td class="blue">52.7</td><td class="blue">66.0</td><td class="blue">84.5</td>
        <td class="blue">69.4</td><td class="blue">91.1</td><td class="blue">72.74</td>
      </tr>

      <tr>
        <td rowspan="3">Qwen2.5-VL</td>
        <td class="red">Qwen2.5-VL-3B-Instruct</td>
        <td class="red">52.0</td><td class="red">47.0</td><td class="red">78.0</td>
        <td class="red">75.0</td><td class="red">74.7</td><td class="red">65.34</td>
      </tr>
      <tr>
        <td>Qwen2.5-VL-7B-Instruct </td>
        <td>64.0</td><td>48.5</td><td>81.4</td><td>71.4</td><td>74.4</td><td>67.94</td>
      </tr>
      <tr>
        <td>Qwen2.5-VL-72B-Instruct </td>
        <td>53.2</td><td>57.2</td><td>85.5</td><td>57.0</td><td>79.1</td><td>66.4</td>
      </tr>

      <tr>
        <td rowspan="2">VideoLLaMA2</td>
        <td>VideoLLaMA2.1-7B</td>
        <td>52.6</td><td>38.3</td><td>80.6</td><td>85.0</td><td>71.7</td><td>65.64</td>
      </tr>
      <tr>
        <td>VideoLLaMA2-72B</td>
        <td>51.8</td><td>46.4</td><td>79.4</td><td>100.0</td><td>77.4</td><td>71.0</td>
      </tr>

      <tr>
        <td rowspan="2">LLaVA-Video</td>
        <td>LLaVA-Video-7B-Qwen2</td>
        <td>49.1</td><td>43.9</td><td>82.3</td><td>90.7</td><td>67.7</td><td>66.74</td>
      </tr>
      <tr>
        <td>LLaVA-Video-72B-Qwen2</td>
        <td>48.9</td><td>51.1</td><td>86.6</td><td>79.5</td><td>76.5</td><td>68.52</td>
      </tr>

      <tr>
        <td rowspan="2">GPT</td>
        <td>GPT-4o-mini</td>
        <td>80.9</td><td>50.4</td><td>79.4</td><td>61.6</td><td>75.4</td><td>69.54</td>
      </tr>
      <tr>
        <td>GPT-4o</td>
        <td>86.5</td><td>57.7</td><td>86.5</td><td>39.4</td><td>80.6</td><td>70.14</td>
      </tr>

      <tr>
        <td>Claude</td>
        <td>Claude-3.5-Sonnet</td>
        <td>98.6</td><td>53.3</td><td>83.6</td><td>42.6</td><td>71.7</td><td>69.96</td>
      </tr>

      <tr>
        <td rowspan="2">Nova</td>
        <td>Nova Lite</td>
        <td>76.5</td><td>41.2</td><td>77.7</td><td>63.6</td><td>68.5</td><td>65.5</td>
      </tr>
      <tr>
        <td>Nova Pro</td>
        <td>78.7</td><td>43.6</td><td>78.5</td><td>85.0</td><td>71.0</td><td>71.36</td>
      </tr>
    </tbody>
  </table>
</div>
        
      </div>
    </div>
  </section>
<!--End leaderboard -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{potter2025vmdt,
  title={VMDT: Decoding the Trustworthiness of Video Foundation Models},
  author = {Potter, Yujin and Wang, Zhun and Crispino, Nicholas and Montgomery, Kyle and Xiong, Alexander and Chang, Ethan Y. and Pinto, Francesco and Chen, Yuqi and Gupta, Rahul and Ziyadi, Morteza and Christodoulopoulos, Christos and Li, Bo and Wang, Chenguang and Song, Dawn},
  journal={Advances in Neural Information Processing Systems},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
